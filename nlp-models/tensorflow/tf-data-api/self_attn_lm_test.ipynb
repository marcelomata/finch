{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/self_attn.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 64,\n",
    "    'text_iter_step': 25,\n",
    "    'seq_len': 200,\n",
    "    'hidden_dim': 128,\n",
    "    'num_head': 8,\n",
    "    'n_hidden_layer': 2,\n",
    "    'display_step': 10,\n",
    "    'generate_step': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(file_path):\n",
    "    with open(file_path) as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    char2idx = {c: i+3 for i, c in enumerate(set(text))}\n",
    "    char2idx['<pad>'] = 0\n",
    "    char2idx['<start>'] = 1\n",
    "    char2idx['<end>'] = 2\n",
    "    \n",
    "    ints = np.array([char2idx[char] for char in list(text)])\n",
    "    return ints, char2idx\n",
    "\n",
    "def next_batch(ints):\n",
    "    len_win = params['seq_len'] * params['batch_size']\n",
    "    for i in range(0, len(ints)-len_win, params['text_iter_step']):\n",
    "        clip = ints[i: i+len_win]\n",
    "        yield clip.reshape([params['batch_size'], params['seq_len']])\n",
    "        \n",
    "def input_fn(ints):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: next_batch(ints), tf.int32, tf.TensorShape([None, params['seq_len']]))\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_sent(x):\n",
    "    _x = tf.fill([tf.shape(x)[0], 1], params['char2idx']['<start>'])\n",
    "    return tf.concat([_x, x], 1)\n",
    "\n",
    "def end_sent(x):\n",
    "    _x = tf.fill([tf.shape(x)[0], 1], params['char2idx']['<end>'])\n",
    "    return tf.concat([x, _x], 1)\n",
    "\n",
    "def embed_seq(x, vocab_sz, embed_dim, name, zero_pad=False, scale=False):\n",
    "    embedding = tf.get_variable(name, [vocab_sz, embed_dim])\n",
    "    if zero_pad:\n",
    "        embedding = tf.concat([tf.zeros([1, embed_dim]), embedding[1:, :]], 0)\n",
    "    x = tf.nn.embedding_lookup(embedding, x)\n",
    "    if scale:\n",
    "        x = x * np.sqrt(embed_dim)\n",
    "    return x\n",
    "\n",
    "def position_embedding(inputs):\n",
    "    T = inputs.get_shape().as_list()[1]\n",
    "    x = tf.range(T)                            # (T)\n",
    "    x = tf.expand_dims(x, 0)                   # (1, T)\n",
    "    x = tf.tile(x, [tf.shape(inputs)[0], 1])   # (N, T)\n",
    "    return embed_seq(x, T, params['hidden_dim'], 'position_embedding')\n",
    "\n",
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "    \n",
    "    params_shape = inputs.get_shape()[-1:]\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    \n",
    "    return gamma * normalized + beta\n",
    "\n",
    "def self_attention(inputs, is_training, activation=None):\n",
    "    num_units = params['hidden_dim']\n",
    "    num_heads = params['num_head']\n",
    "    T_q = T_k = inputs.get_shape().as_list()[1]\n",
    "\n",
    "    Q_K_V = tf.layers.dense(inputs, 3*num_units, activation)\n",
    "    Q, K, V = tf.split(Q_K_V, 3, -1)\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), 0)                         \n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), 0)                        \n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), 0)                         \n",
    "\n",
    "    align = tf.matmul(Q_, tf.transpose(K_, [0,2,1]))                               \n",
    "    align = align / np.sqrt(K_.get_shape().as_list()[-1])\n",
    "\n",
    "    paddings = tf.fill(tf.shape(align), float('-inf'))         \n",
    "    lower_tri = tf.ones([T_q, T_k])                                                \n",
    "    lower_tri = tf.linalg.LinearOperatorLowerTriangular(lower_tri).to_dense()      \n",
    "    masks = tf.tile(tf.expand_dims(lower_tri,0), [tf.shape(align)[0],1,1])       \n",
    "    align = tf.where(tf.equal(masks, 0), paddings, align)               \n",
    "\n",
    "    align = tf.nn.softmax(align)                                                  \n",
    "    align = tf.layers.dropout(align, 0.1, training=is_training)           \n",
    "    x = tf.matmul(align, V_)                                                 \n",
    "    x = tf.concat(tf.split(x, num_heads, axis=0), 2)              \n",
    "    x += inputs                                                                \n",
    "    x = layer_norm(x)                                                 \n",
    "    return x\n",
    "\n",
    "def ffn(inputs, activation=tf.nn.relu):\n",
    "    x = tf.layers.conv1d(inputs, 4*params['hidden_dim'], 1, activation=activation)\n",
    "    x = tf.layers.conv1d(x, params['hidden_dim'], 1, activation=None)\n",
    "    x += inputs\n",
    "    x = layer_norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, reuse, is_training):\n",
    "    inputs = start_sent(inputs)\n",
    "    with tf.variable_scope('model', reuse=reuse):\n",
    "        x = embed_seq(inputs, params['vocab_size'], params['hidden_dim'], 'word_embedding',\n",
    "                      zero_pad=True, scale=True)\n",
    "        x += position_embedding(x)\n",
    "        x = tf.layers.dropout(x, 0.1, training=is_training)\n",
    "        \n",
    "        for i in range(params['n_hidden_layer']):\n",
    "            with tf.variable_scope('attn_%d'%i, reuse=reuse):\n",
    "                x = self_attention(x, is_training)\n",
    "            with tf.variable_scope('ffn_%d'%i, reuse=reuse):\n",
    "                x = ffn(x)\n",
    "        \n",
    "        logits = tf.layers.dense(x, params['vocab_size'])\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive():\n",
    "    def cond(i, x, temp):\n",
    "        return i < params['seq_len']\n",
    "\n",
    "    def body(i, x, temp):\n",
    "        logits = forward(x, reuse=True, is_training=False)\n",
    "        ids = tf.argmax(logits, -1, output_type=tf.int32)[:, i]\n",
    "        ids = tf.expand_dims(ids, -1)\n",
    "\n",
    "        temp = tf.concat([temp[:, 1:], ids], -1)\n",
    "\n",
    "        x = tf.concat([temp[:, -(i+1):], temp[:, :-(i+1)]], -1)\n",
    "        x = tf.reshape(x, [1, params['seq_len']])\n",
    "        i += 1\n",
    "        return i, x, temp\n",
    "\n",
    "    x = tf.zeros([1, params['seq_len']], tf.int32)\n",
    "    _, res, _ = tf.while_loop(cond, body, [tf.constant(0), x, x])\n",
    "    \n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 86\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "ints, params['char2idx'] = parse_text('../temp/anna.txt')\n",
    "params['vocab_size'] = len(params['char2idx'])\n",
    "params['idx2char'] = {i: c for c, i in params['char2idx'].items()}\n",
    "print('Vocabulary size:', params['vocab_size'])\n",
    "\n",
    "X = input_fn(ints)\n",
    "logits = forward(X, reuse=False, is_training=True)\n",
    "\n",
    "ops = {}\n",
    "ops['global_step'] = tf.Variable(0, trainable=False)\n",
    "\n",
    "targets = end_sent(X)\n",
    "ops['loss'] = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(\n",
    "    logits = logits,\n",
    "    targets = targets,\n",
    "    weights = tf.to_float(tf.ones_like(targets))))\n",
    "\n",
    "ops['train'] = tf.train.AdamOptimizer().minimize(ops['loss'], global_step=ops['global_step'])\n",
    "\n",
    "ops['generate'] = autoregressive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Loss 3.928\n",
      "Step 10 | Loss 2.773\n",
      "Step 20 | Loss 2.598\n",
      "Step 30 | Loss 2.524\n",
      "Step 40 | Loss 2.486\n",
      "Step 50 | Loss 2.462\n",
      "Step 60 | Loss 2.435\n",
      "Step 70 | Loss 2.416\n",
      "Step 80 | Loss 2.395\n",
      "Step 90 | Loss 2.384\n",
      "Step 100 | Loss 2.366\n",
      "\n",
      "and the he he he he he he he he he he he he he he he he he he han he he he he he he he he han he he he he he he he he he he han he he he he he he he he he he he he he he he he he he he he he he han he\n",
      "\n",
      "Step 110 | Loss 2.348\n",
      "Step 120 | Loss 2.334\n",
      "Step 130 | Loss 2.311\n",
      "Step 140 | Loss 2.293\n",
      "Step 150 | Loss 2.271\n",
      "Step 160 | Loss 2.254\n",
      "Step 170 | Loss 2.242\n",
      "Step 180 | Loss 2.213\n",
      "Step 190 | Loss 2.191\n",
      "Step 200 | Loss 2.174\n",
      "\n",
      " and the the the the is he he he he se se se he se sean the the se the cofon he han he he he han hathe he he he he hand he se he he he se se se hand se se the se he suse se thatheathe se hathe suse th\n",
      "\n",
      "Step 210 | Loss 2.161\n",
      "Step 220 | Loss 2.151\n",
      "Step 230 | Loss 2.119\n",
      "Step 240 | Loss 2.103\n",
      "Step 250 | Loss 2.096\n",
      "Step 260 | Loss 2.070\n",
      "Step 270 | Loss 2.056\n",
      "Step 280 | Loss 2.034\n",
      "Step 290 | Loss 2.009\n",
      "Step 300 | Loss 2.010\n",
      "\n",
      " and the se ther se isthe ofacond his wat he he he handis hat hat he hat s ofat hat he he lise he hat he se se hat se serofat at he he at hat he at at at he atorrond hat athat hat lise t he at herouse\n",
      "\n",
      "Step 310 | Loss 1.975\n",
      "Step 320 | Loss 1.965\n",
      "Step 330 | Loss 1.944\n",
      "Step 340 | Loss 1.932\n",
      "Step 350 | Loss 1.899\n",
      "Step 360 | Loss 1.890\n",
      "Step 370 | Loss 1.898\n",
      "Step 380 | Loss 1.866\n",
      "Step 390 | Loss 1.834\n",
      "Step 400 | Loss 1.817\n",
      "\n",
      " and ther sthe se stouse to the the t the ther se tathe se se tatouse tout to the the\n",
      "the t the ouse t the ouse the touse touse touse touthe t to the toun the touse to the touthe to ouse\n",
      "the the the t\n",
      "\n",
      "Step 410 | Loss 1.802\n",
      "Step 420 | Loss 1.783\n",
      "Step 430 | Loss 1.803\n",
      "Step 440 | Loss 1.777\n",
      "Step 450 | Loss 1.760\n",
      "Step 460 | Loss 1.732\n",
      "Step 470 | Loss 1.703\n",
      "Step 480 | Loss 1.705\n",
      "Step 490 | Loss 1.678\n",
      "Step 500 | Loss 1.644\n",
      "\n",
      " hat she sof his le sore sof r sor this se sof re aling hing s\n",
      "rereread ht sofom his s s sof his acong soalisert his of his s s and hisererer lis, and and\n",
      "fraling hing htor or ser sof hiner ser somina\n",
      "\n",
      "Step 510 | Loss 1.634\n",
      "Step 520 | Loss 1.612\n",
      "Step 530 | Loss 1.604\n",
      "Step 540 | Loss 1.575\n",
      "Step 550 | Loss 1.553\n",
      "Step 560 | Loss 1.539\n",
      "Step 570 | Loss 1.506\n",
      "Step 580 | Loss 1.490\n",
      "Step 590 | Loss 1.475\n",
      "Step 600 | Loss 1.437\n",
      "\n",
      " her face and\n",
      "my ther wasoun the the to ther ther touldr the tougher tought oun t the\n",
      "wo thouldr------youroun.\n",
      "\n",
      "\"You s s is could cone cone the couldry thinge tonger one the of ouldr nce\n",
      "the the wis o\n",
      "\n",
      "Step 610 | Loss 1.438\n",
      "Step 620 | Loss 1.475\n",
      "Step 630 | Loss 1.394\n",
      "Step 640 | Loss 1.382\n",
      "Step 650 | Loss 1.348\n",
      "Step 660 | Loss 1.330\n",
      "Step 670 | Loss 1.325\n",
      "Step 680 | Loss 1.326\n",
      "Step 690 | Loss 1.294\n",
      "Step 700 | Loss 1.273\n",
      "\n",
      " she sad werend to\n",
      "lunthe ther for ther sthe fand the he was as as sond\n",
      "luping on othing other other had\n",
      "her days ways don to to to her. He r thave the gooner this fffaner sptain's widow,\n",
      "Stepan Arkad\n",
      "\n",
      "Step 710 | Loss 1.250\n",
      "Step 720 | Loss 1.237\n",
      "Step 730 | Loss 1.242\n",
      "Step 740 | Loss 1.218\n",
      "Step 750 | Loss 1.231\n",
      "Step 760 | Loss 1.203\n",
      "Step 770 | Loss 1.198\n",
      "Step 780 | Loss 1.187\n",
      "Step 790 | Loss 1.151\n",
      "Step 800 | Loss 1.155\n",
      "\n",
      " she hould gonot othis her had rowner hin the he had ger atof\n",
      "be.\n",
      "\n",
      "\"Yes, lou hones lones me lovesesone!\" shoughtid, shoughtt. \"And shr.\"\n",
      "\n",
      "\n",
      "\"Youre whare lloure shat med sthourredily shries king ooomewh\n",
      "\n",
      "Step 810 | Loss 1.156\n",
      "Step 820 | Loss 1.116\n",
      "Step 830 | Loss 1.114\n",
      "Step 840 | Loss 1.118\n",
      "Step 850 | Loss 1.101\n",
      "Step 860 | Loss 1.092\n",
      "Step 870 | Loss 1.085\n",
      "Step 880 | Loss 1.065\n",
      "Step 890 | Loss 1.069\n",
      "Step 900 | Loss 1.053\n",
      "\n",
      " same, and which wat ame the was ing in he the is\n",
      "rdathe beeeeed a d in alk end to her\n",
      "word by fileng him the hane dr athis, begand wot and any he\n",
      "puck frorom his his ate, and was he he bend lack, \"ma\n",
      "\n",
      "Step 910 | Loss 1.050\n",
      "Step 920 | Loss 1.030\n",
      "Step 930 | Loss 1.041\n",
      "Step 940 | Loss 1.025\n",
      "Step 950 | Loss 1.014\n",
      "Step 960 | Loss 1.002\n",
      "Step 970 | Loss 1.013\n",
      "Step 980 | Loss 0.998\n",
      "Step 990 | Loss 0.983\n",
      "Step 1000 | Loss 0.986\n",
      "\n",
      " the sad went the salas, wh gen as ind the the gering the things ny sand wall tof wecas s\n",
      "rigndiffring hidly, nover the ad owa s do the sat de go dof the\n",
      "ooown bent stalo it itig iof her se was cas im\n",
      "\n",
      "Step 1010 | Loss 0.983\n",
      "Step 1020 | Loss 0.956\n",
      "Step 1030 | Loss 0.954\n",
      "Step 1040 | Loss 0.956\n",
      "Step 1050 | Loss 0.936\n",
      "Step 1060 | Loss 0.927\n",
      "Step 1070 | Loss 0.933\n",
      "Step 1080 | Loss 0.927\n",
      "Step 1090 | Loss 0.922\n",
      "Step 1100 | Loss 0.929\n",
      "\n",
      " the saind the reabes, and whin the too se alithis the realmpe act acthe ale almsssige,\n",
      "hed, and ighen wasthim the thate do then rving herim and thiming the too\n",
      "dofice surd andeveryoner ther, and by v\n",
      "\n",
      "Step 1110 | Loss 0.936\n",
      "Step 1120 | Loss 0.943\n",
      "Step 1130 | Loss 0.927\n",
      "Step 1140 | Loss 0.921\n",
      "Step 1150 | Loss 0.899\n",
      "Step 1160 | Loss 0.918\n",
      "Step 1170 | Loss 0.908\n",
      "Step 1180 | Loss 0.878\n",
      "Step 1190 | Loss 0.893\n",
      "Step 1200 | Loss 0.881\n",
      "\n",
      " the sand recoly ou, butord bersomele\n",
      "d to of el this of se the pearectely llew\n",
      "hongener ly laraned ll mofesein the ffelbor arindltll thim, and bus wh friend his friends offriends of of the\n",
      "riers ome \n",
      "\n",
      "Step 1210 | Loss 0.887\n",
      "Step 1220 | Loss 0.872\n",
      "Step 1230 | Loss 0.839\n",
      "Step 1240 | Loss 0.870\n",
      "Step 1250 | Loss 0.835\n",
      "Step 1260 | Loss 0.836\n",
      "Step 1270 | Loss 0.852\n",
      "Step 1280 | Loss 0.822\n",
      "Step 1290 | Loss 0.816\n",
      "Step 1300 | Loss 0.814\n",
      "\n",
      " the same tonthe his had tate threims.\n",
      "\n",
      "\"Wellll, t Levevin aftinch shat bot ce?\" said Stepan Arkadyevitch, and wand his having he t\n",
      "de istoong he kn without r sund of amune ch Oblonsky, which\n",
      "s ficomp\n",
      "\n",
      "Step 1310 | Loss 0.835\n",
      "Step 1320 | Loss 0.809\n",
      "Step 1330 | Loss 0.806\n",
      "Step 1340 | Loss 0.809\n",
      "Step 1350 | Loss 0.782\n",
      "Step 1360 | Loss 0.773\n",
      "Step 1370 | Loss 0.797\n",
      "Step 1380 | Loss 0.781\n",
      "Step 1390 | Loss 0.768\n",
      "Step 1400 | Loss 0.779\n",
      "\n",
      " the same hagrered angret the at ssany friends.\n",
      "\n",
      "\n",
      "\"You said a few words, but I can't a a and t wer y worelu,\" hat hat hat ck soure we's, t a me a do the knowing his\n",
      "his imapag.\n",
      "\n",
      "\"Obl te sact onsll se.\n",
      "\n",
      "Step 1410 | Loss 0.792\n",
      "Step 1420 | Loss 0.783\n",
      "Step 1430 | Loss 0.781\n",
      "Step 1440 | Loss 0.777\n",
      "Step 1450 | Loss 0.779\n",
      "Step 1460 | Loss 0.779\n",
      "Step 1470 | Loss 0.751\n",
      "Step 1480 | Loss 0.745\n",
      "Step 1490 | Loss 0.759\n",
      "Step 1500 | Loss 0.744\n",
      "\n",
      " the sand the sown ay. \"Well,\n",
      "how are you? Eh? When did you come?\"\n",
      "\n",
      "Levin was silent, looking at the unknown faces of Oblonsky's two\n",
      "companions, and especially at the hand of the elvess amely freeence\n",
      "\n",
      "Step 1510 | Loss 0.745\n",
      "Step 1520 | Loss 0.747\n",
      "Step 1530 | Loss 0.759\n",
      "Step 1540 | Loss 0.760\n",
      "Step 1550 | Loss 0.753\n",
      "Step 1560 | Loss 0.741\n",
      "Step 1570 | Loss 0.739\n",
      "Step 1580 | Loss 0.735\n",
      "Step 1590 | Loss 0.726\n",
      "Step 1600 | Loss 0.728\n",
      "\n",
      " hat the at had soly ad come to quion, and cand his on's oa tay lo feelor.\"\n",
      "\n",
      "\"I have you you and quitit ang the to this wallong\n",
      "to e eaf eamate, the mamer was as in lof thalo he coune lard the hat cou\n",
      "\n",
      "Step 1610 | Loss 0.732\n",
      "Step 1620 | Loss 0.745\n",
      "Step 1630 | Loss 0.759\n",
      "Step 1640 | Loss 0.740\n",
      "Step 1650 | Loss 0.721\n",
      "Step 1660 | Loss 0.740\n",
      "Step 1670 | Loss 0.737\n",
      "Step 1680 | Loss 0.727\n",
      "Step 1690 | Loss 0.715\n",
      "Step 1700 | Loss 0.739\n",
      "\n",
      " the a conce w ablusinte in their brother's room above, where the students\n",
      "used to work; why they were visited by those professors of French\n",
      "literature, of music, of drawing, of dancing; why at certai\n",
      "\n",
      "Step 1710 | Loss 0.705\n",
      "Step 1720 | Loss 0.704\n",
      "Step 1730 | Loss 0.702\n",
      "Step 1740 | Loss 0.706\n",
      "Step 1750 | Loss 0.701\n",
      "Step 1760 | Loss 0.717\n",
      "Step 1770 | Loss 0.686\n",
      "Step 1780 | Loss 0.709\n",
      "Step 1790 | Loss 0.684\n",
      "Step 1800 | Loss 0.688\n",
      "\n",
      " the brot himster puty, was had so pronsot besshe of had cor\n",
      "is thentirs f brother of began man his the conversate.\n",
      "\n",
      "ctiould ameng at ily thathe st he could not himsent the be implegan arenid the the \n",
      "\n",
      "Step 1810 | Loss 0.693\n",
      "Step 1820 | Loss 0.710\n",
      "Step 1830 | Loss 0.693\n",
      "Step 1840 | Loss 0.704\n",
      "Step 1850 | Loss 0.685\n",
      "Step 1860 | Loss 0.681\n",
      "Step 1870 | Loss 0.661\n",
      "Step 1880 | Loss 0.655\n",
      "Step 1890 | Loss 0.668\n",
      "Step 1900 | Loss 0.667\n",
      "\n",
      " sen istion the dediscud.\n",
      "\n",
      "\n",
      "Levin revin do con hace t upld ncessed by began to get ich thered id sctonceion questions with those spiritual\n",
      "problems, that at times they almost they tourn been in love w\n",
      "\n",
      "Step 1910 | Loss 0.661\n",
      "Step 1920 | Loss 0.659\n",
      "Step 1930 | Loss 0.627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1940 | Loss 0.645\n",
      "Step 1950 | Loss 0.656\n",
      "Step 1960 | Loss 0.660\n",
      "Step 1970 | Loss 0.642\n",
      "Step 1980 | Loss 0.649\n",
      "Step 1990 | Loss 0.657\n",
      "Step 2000 | Loss 0.647\n",
      "\n",
      " as as hart at you would dordis, wifith anded\n",
      "adis that he und noto Oblonsky, whe mar ther\n",
      "way proferour hat ad atter the tatired ap turon a phile int his my to\n",
      "whim. Buter the connection wed he that \n",
      "\n",
      "Step 2010 | Loss 0.654\n",
      "Step 2020 | Loss 0.632\n",
      "Step 2030 | Loss 0.629\n",
      "Step 2040 | Loss 0.656\n",
      "Step 2050 | Loss 0.666\n",
      "Step 2060 | Loss 0.653\n",
      "Step 2070 | Loss 0.657\n",
      "Step 2080 | Loss 0.638\n",
      "Step 2090 | Loss 0.632\n",
      "Step 2100 | Loss 0.647\n",
      "\n",
      " that he his\n",
      "yould mer un they questin dis hner ot begice him.\n",
      "\n",
      "\"I cant sanot to the de nof asent o meent Levin went he to profent she prort, and she went\n",
      "back to his argument her, and the tin own shi\n",
      "\n",
      "Step 2110 | Loss 0.651\n",
      "Step 2120 | Loss 0.643\n",
      "Step 2130 | Loss 0.651\n",
      "Step 2140 | Loss 0.650\n",
      "Step 2150 | Loss 0.642\n",
      "Step 2160 | Loss 0.649\n",
      "Step 2170 | Loss 0.653\n",
      "Step 2180 | Loss 0.627\n",
      "Step 2190 | Loss 0.634\n",
      "Step 2200 | Loss 0.627\n",
      "\n",
      " as as the of thee suncins of the skate ting rof oothe of his on dreconf se, as orthe skaters, he kate.\"\n",
      "\n",
      "\"Yow he knou the stin,\" Levin regout and weld hing at her. She skated a littlle funding an in \n",
      "\n",
      "Step 2210 | Loss 0.617\n",
      "Step 2220 | Loss 0.629\n",
      "Step 2230 | Loss 0.633\n",
      "Step 2240 | Loss 0.617\n",
      "Step 2250 | Loss 0.621\n",
      "Step 2260 | Loss 0.603\n",
      "Step 2270 | Loss 0.596\n",
      "Step 2280 | Loss 0.610\n",
      "Step 2290 | Loss 0.611\n",
      "Step 2300 | Loss 0.613\n",
      "\n",
      " saided, with\n",
      "perfect self-possession, skated towards her, skated by her, even spoke\n",
      "to her, and were happy, quite apart from her, enjoying the capital ice\n",
      "and the fine weather.\n",
      "\n",
      "Nikolay Shtcherbatsky\n",
      "\n",
      "Step 2310 | Loss 0.619\n",
      "Step 2320 | Loss 0.617\n",
      "Step 2330 | Loss 0.606\n",
      "Step 2340 | Loss 0.594\n",
      "Step 2350 | Loss 0.602\n",
      "Step 2360 | Loss 0.601\n",
      "Step 2370 | Loss 0.617\n",
      "Step 2380 | Loss 0.621\n",
      "Step 2390 | Loss 0.618\n",
      "Step 2400 | Loss 0.597\n",
      "\n",
      " the sart sumid and at the skatering together and the oppoachaiartidir a sias nd\n",
      "fustmilend he wor re to rane the\n",
      "ice, go ande to the the was wing. The tradition ith se heart skates, I'ming ood and se\n",
      "\n",
      "Step 2410 | Loss 0.602\n",
      "Step 2420 | Loss 0.594\n",
      "Step 2430 | Loss 0.585\n",
      "Step 2440 | Loss 0.575\n",
      "Step 2450 | Loss 0.584\n",
      "Step 2460 | Loss 0.582\n",
      "Step 2470 | Loss 0.601\n",
      "Step 2480 | Loss 0.590\n",
      "Step 2490 | Loss 0.611\n",
      "Step 2500 | Loss 0.608\n",
      "\n",
      " a cord, and herthe gat Frenchwoman with ga ith grout people the which would overspreares he he had bowith oldidith st cate mowaye re the was he was ords, when all frondere and with hought her. \"Ehad,\n",
      "\n",
      "Step 2510 | Loss 0.594\n",
      "Step 2520 | Loss 0.608\n",
      "Step 2530 | Loss 0.614\n",
      "Step 2540 | Loss 0.592\n",
      "Step 2550 | Loss 0.600\n",
      "Step 2560 | Loss 0.587\n",
      "Step 2570 | Loss 0.587\n",
      "Step 2580 | Loss 0.583\n",
      "Step 2590 | Loss 0.574\n",
      "Step 2600 | Loss 0.588\n",
      "\n",
      " that the was nothe something voter\n",
      "we thad the tak to one he smile seelf in cher and daughter\n",
      "at the al of the skate. \"Except you,\n",
      "there's none of the gentlemen first-rate skaters. Will that be all\n",
      "r\n",
      "\n",
      "Step 2610 | Loss 0.588\n",
      "Step 2620 | Loss 0.562\n",
      "Step 2630 | Loss 0.572\n",
      "Step 2640 | Loss 0.556\n",
      "Step 2650 | Loss 0.551\n",
      "Step 2660 | Loss 0.556\n",
      "Step 2670 | Loss 0.555\n",
      "Step 2680 | Loss 0.557\n",
      "Step 2690 | Loss 0.562\n",
      "Step 2700 | Loss 0.563\n",
      "\n",
      " the coman, and he whe with a violent effort recovered himself, and skated off,\n",
      "laughing.\n",
      "\n",
      "\"How splendid, how nice he is!\" Kitty was thinking at that time, as she\n",
      "came out of the pavilion with Mlle. L\n",
      "\n",
      "Step 2710 | Loss 0.551\n",
      "Step 2720 | Loss 0.558\n",
      "Step 2730 | Loss 0.568\n",
      "Step 2740 | Loss 0.561\n",
      "Step 2750 | Loss 0.550\n",
      "Step 2760 | Loss 0.527\n",
      "Step 2770 | Loss 0.522\n",
      "Step 2780 | Loss 0.509\n",
      "Step 2790 | Loss 0.510\n",
      "Step 2800 | Loss 0.506\n",
      "\n",
      " the cowll anged ther him there reming that brand behand onde he cling the bin is will, gand of ther for, and ive excellency, please. Your excellency won't be disturbed\n",
      "here,\" said a particularly pert\n",
      "\n",
      "Step 2810 | Loss 0.523\n",
      "Step 2820 | Loss 0.519\n",
      "Step 2830 | Loss 0.506\n",
      "Step 2840 | Loss 0.510\n",
      "Step 2850 | Loss 0.519\n",
      "Step 2860 | Loss 0.524\n",
      "Step 2870 | Loss 0.521\n",
      "Step 2880 | Loss 0.522\n",
      "Step 2890 | Loss 0.511\n",
      "Step 2900 | Loss 0.518\n",
      "\n",
      " and that tat the sid,\n",
      "but I saw ee youre up nott, you mut the for a done thing so world except her, and those girls with\n",
      "all sorts of human weaknesses, and very ordinary girls: the other\n",
      "class--she a\n",
      "\n",
      "Step 2910 | Loss 0.512\n",
      "Step 2920 | Loss 0.509\n",
      "Step 2930 | Loss 0.518\n",
      "Step 2940 | Loss 0.513\n",
      "Step 2950 | Loss 0.502\n",
      "Step 2960 | Loss 0.526\n",
      "Step 2970 | Loss 0.531\n",
      "Step 2980 | Loss 0.520\n",
      "Step 2990 | Loss 0.512\n",
      "Step 3000 | Loss 0.508\n",
      "\n",
      " one can't help feeling oneself unworthy.\"\n",
      "\n",
      "\"Why the ing ive to the for mets just as hing;t bat iland a spring this\n",
      "to down peabout your in a sparkling wine into the delicate\n",
      "glasses, glanced at Stepa\n",
      "\n",
      "Step 3010 | Loss 0.516\n",
      "Step 3020 | Loss 0.489\n",
      "Step 3030 | Loss 0.477\n",
      "Step 3040 | Loss 0.479\n",
      "Step 3050 | Loss 0.498\n",
      "Step 3060 | Loss 0.549\n",
      "Step 3070 | Loss 0.510\n",
      "Step 3080 | Loss 0.500\n",
      "Step 3090 | Loss 0.507\n",
      "Step 3100 | Loss 0.510\n",
      "\n",
      " one can't hounw hate I ought to know. The only thing I know is that you\n",
      "always do what no one else does.\"\n",
      "\n",
      "\"Yes,\" said Levin, slowly and with emotion, \"you're right. I am a\n",
      "savage. Only, my savagenes\n",
      "\n",
      "Step 3110 | Loss 0.500\n",
      "Step 3120 | Loss 0.514\n",
      "Step 3130 | Loss 0.498\n",
      "Step 3140 | Loss 0.491\n",
      "Step 3150 | Loss 0.487\n",
      "Step 3160 | Loss 0.497\n",
      "Step 3170 | Loss 0.520\n",
      "Step 3180 | Loss 0.477\n",
      "Step 3190 | Loss 0.480\n",
      "Step 3200 | Loss 0.485\n",
      "\n",
      " me.\"\n",
      "\n",
      "\n",
      "\n",
      "Chapter 11\n",
      "\n",
      "\n",
      "Levin emptied his glass, and they were silent for a while.\n",
      "\n",
      "\"There's one other thing I ought to tell you. Do you know Vronsky?\"\n",
      "Stepan Arkadyevitch asked Levin.\n",
      "\n",
      "\"No, I don't. Wh\n",
      "\n",
      "Step 3210 | Loss 0.469\n",
      "Step 3220 | Loss 0.488\n",
      "Step 3230 | Loss 0.474\n",
      "Step 3240 | Loss 0.506\n",
      "Step 3250 | Loss 0.498\n",
      "Step 3260 | Loss 0.511\n",
      "Step 3270 | Loss 0.488\n",
      "Step 3280 | Loss 0.496\n",
      "Step 3290 | Loss 0.482\n",
      "Step 3300 | Loss 0.483\n",
      "\n",
      " oneself! Listen. You can't imagine what you've\n",
      "done for me by what you said. I'm so happy that I've become positively\n",
      "hateful; I've forgotten everything. I heard today that my brother\n",
      "Nikolay ... you\n",
      "\n",
      "Step 3310 | Loss 0.495\n",
      "Step 3320 | Loss 0.489\n",
      "Step 3330 | Loss 0.499\n",
      "Step 3340 | Loss 0.500\n",
      "Step 3350 | Loss 0.512\n",
      "Step 3360 | Loss 0.507\n",
      "Step 3370 | Loss 0.504\n",
      "Step 3380 | Loss 0.513\n",
      "Step 3390 | Loss 0.491\n",
      "Step 3400 | Loss 0.498\n",
      "\n",
      " all she my come are thing. Now, when the thing's done, don't\n",
      "you see, can one possibly cast her off? Even supposing one parts from\n",
      "her, so as not to break up one's family life, still, can one help\n",
      "fe\n",
      "\n",
      "Step 3410 | Loss 0.499\n",
      "Step 3420 | Loss 0.496\n",
      "Step 3430 | Loss 0.506\n",
      "Step 3440 | Loss 0.496\n",
      "Step 3450 | Loss 0.486\n",
      "Step 3460 | Loss 0.491\n",
      "Step 3470 | Loss 0.487\n",
      "Step 3480 | Loss 0.478\n",
      "Step 3490 | Loss 0.485\n",
      "Step 3500 | Loss 0.498\n",
      "\n",
      " of girls, Dary men's st and so mankeaw.\"\n",
      "\n",
      "When Vronsky appeared on the scene, she was still more delighted,\n",
      "confirmed in her opinion that Kitty was to make not simply a good, but a\n",
      "brilliant match.\n",
      "\n",
      "\n",
      "\n",
      "Step 3510 | Loss 0.498\n",
      "Step 3520 | Loss 0.499\n",
      "Step 3530 | Loss 0.484\n",
      "Step 3540 | Loss 0.488\n",
      "Step 3550 | Loss 0.487\n",
      "Step 3560 | Loss 0.494\n",
      "Step 3570 | Loss 0.485\n",
      "Step 3580 | Loss 0.474\n",
      "Step 3590 | Loss 0.492\n",
      "Step 3600 | Loss 0.481\n",
      "\n",
      " failing. You have a\n",
      "character that's all of a piece, and you want the whole of life to be of\n",
      "a piece too--but that's not how it is. You despise public official work\n",
      "because you want the reality to be\n",
      "\n",
      "Step 3610 | Loss 0.482\n",
      "Step 3620 | Loss 0.482\n",
      "Step 3630 | Loss 0.486\n",
      "Step 3640 | Loss 0.485\n",
      "Step 3650 | Loss 0.486\n",
      "Step 3660 | Loss 0.487\n",
      "Step 3670 | Loss 0.467\n",
      "Step 3680 | Loss 0.469\n",
      "Step 3690 | Loss 0.482\n",
      "Step 3700 | Loss 0.485\n",
      "\n",
      " as the fure be, pistranced, as the she itupe was pritaincess so bettion of the ar convinced that to choose their husbands was their own\n",
      "affair, and not their parents'. \"Marriages aren't made nowadays\n",
      "\n",
      "Step 3710 | Loss 0.465\n",
      "Step 3720 | Loss 0.465\n",
      "Step 3730 | Loss 0.454\n",
      "Step 3740 | Loss 0.455\n",
      "Step 3750 | Loss 0.449\n",
      "Step 3760 | Loss 0.457\n",
      "Step 3770 | Loss 0.470\n",
      "Step 3780 | Loss 0.464\n",
      "Step 3790 | Loss 0.473\n",
      "Step 3800 | Loss 0.475\n",
      "\n",
      " as the princess of\n",
      "getting to know each other, her daughter might fall in love, and fall in\n",
      "love with someone who did not care to marry her or who was quite unfit\n",
      "to be her husband. And, however much\n",
      "\n",
      "Step 3810 | Loss 0.466\n",
      "Step 3820 | Loss 0.470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3830 | Loss 0.479\n",
      "Step 3840 | Loss 0.483\n",
      "Step 3850 | Loss 0.480\n",
      "Step 3860 | Loss 0.479\n",
      "Step 3870 | Loss 0.488\n",
      "Step 3880 | Loss 0.451\n",
      "Step 3890 | Loss 0.454\n",
      "Step 3900 | Loss 0.454\n",
      "\n",
      " the at of her other make alone, and their disturbed faces. Levin\n",
      "bowed to her, and said nothing. Kitty did not speak nor lift her eyes.\n",
      "\"Thank God, she has refused him,\" thought the mother, and her f\n",
      "\n",
      "Step 3910 | Loss 0.438\n",
      "Step 3920 | Loss 0.441\n",
      "Step 3930 | Loss 0.452\n",
      "Step 3940 | Loss 0.447\n",
      "Step 3950 | Loss 0.474\n",
      "Step 3960 | Loss 0.463\n",
      "Step 3970 | Loss 0.467\n",
      "Step 3980 | Loss 0.466\n",
      "Step 3990 | Loss 0.455\n",
      "Step 4000 | Loss 0.443\n",
      "\n",
      " as surely in and cannot even be offended by each\n",
      "other.\n",
      "\n",
      "The Countess Nordston pounced upon Levin at once.\n",
      "\n",
      "\"Ah, Konstantin Dmitrievitch! So you've come back to our corrupt\n",
      "Babylon,\" she said, giving\n",
      "\n",
      "Step 4010 | Loss 0.437\n",
      "Step 4020 | Loss 0.448\n",
      "Step 4030 | Loss 0.459\n",
      "Step 4040 | Loss 0.439\n",
      "Step 4050 | Loss 0.454\n",
      "Step 4060 | Loss 0.449\n",
      "Step 4070 | Loss 0.462\n",
      "Step 4080 | Loss 0.436\n",
      "Step 4090 | Loss 0.453\n",
      "Step 4100 | Loss 0.430\n",
      "\n",
      " with a faint, happy, and modestly triumphant smile (so it\n",
      "seemed to Levin), bowing carefully and respectfully over her, he held\n",
      "out his small broad hand to her.\n",
      "\n",
      "Greeting and saying a few words to ev\n",
      "\n",
      "Step 4110 | Loss 0.423\n",
      "Step 4120 | Loss 0.441\n",
      "Step 4130 | Loss 0.443\n",
      "Step 4140 | Loss 0.433\n",
      "Step 4150 | Loss 0.432\n",
      "Step 4160 | Loss 0.449\n",
      "Step 4170 | Loss 0.445\n",
      "Step 4180 | Loss 0.450\n",
      "Step 4190 | Loss 0.419\n",
      "Step 4200 | Loss 0.428\n",
      "\n",
      " she saying about the most the mothing of it. You know all about such things. At home in our\n",
      "village of Kaluga all the peasants and all the women have drunk up all\n",
      "they possessed, and now they can't p\n",
      "\n",
      "Step 4210 | Loss 0.438\n",
      "Step 4220 | Loss 0.436\n",
      "Step 4230 | Loss 0.434\n",
      "Step 4240 | Loss 0.431\n",
      "Step 4250 | Loss 0.425\n",
      "Step 4260 | Loss 0.428\n",
      "Step 4270 | Loss 0.402\n",
      "Step 4280 | Loss 0.410\n",
      "Step 4290 | Loss 0.423\n",
      "Step 4300 | Loss 0.434\n",
      "\n",
      " she with his bee\n",
      "address all to find in what love in his.\n",
      "\n",
      "\"I don't know; I have never tried for long. I experienced a queer\n",
      "feeling once,\" he went on. \"I never longed so for the country, Russian\n",
      "cou\n",
      "\n",
      "Step 4310 | Loss 0.454\n",
      "Step 4320 | Loss 0.465\n",
      "Step 4330 | Loss 0.454\n",
      "Step 4340 | Loss 0.438\n",
      "Step 4350 | Loss 0.430\n",
      "Step 4360 | Loss 0.441\n",
      "Step 4370 | Loss 0.444\n",
      "Step 4380 | Loss 0.435\n",
      "Step 4390 | Loss 0.454\n",
      "Step 4400 | Loss 0.430\n",
      "\n",
      " her eyes. But immediately she thought of the man for whom she had\n",
      "given him up. She vividly recalled his manly, resolute face, his noble\n",
      "self-possession, and the good nature conspicuous in everything\n",
      "\n",
      "Step 4410 | Loss 0.432\n",
      "Step 4420 | Loss 0.430\n",
      "Step 4430 | Loss 0.419\n",
      "Step 4440 | Loss 0.429\n",
      "Step 4450 | Loss 0.436\n",
      "Step 4460 | Loss 0.443\n",
      "Step 4470 | Loss 0.444\n",
      "Step 4480 | Loss 0.433\n",
      "Step 4490 | Loss 0.423\n",
      "Step 4500 | Loss 0.425\n",
      "\n",
      " he ladies.\n",
      "\n",
      "\"Do let us try at once, countess,\" he said; but Levin would finish\n",
      "saying what he thought.\n",
      "\n",
      "\"I think,\" he went on, \"that this attempt of the spiritualists to\n",
      "explain their marvels as some\n",
      "\n",
      "Step 4510 | Loss 0.416\n",
      "Step 4520 | Loss 0.420\n",
      "Step 4530 | Loss 0.417\n",
      "Step 4540 | Loss 0.426\n",
      "Step 4550 | Loss 0.426\n",
      "Step 4560 | Loss 0.437\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "while True:\n",
    "    try:\n",
    "        sess.run(ops['train'])\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "    else:\n",
    "        step = sess.run(ops['global_step'])\n",
    "        if step % params['display_step'] == 0 or step == 1:\n",
    "            loss = sess.run(ops['loss'])\n",
    "            print(\"Step %d | Loss %.3f\" % (step, loss))\n",
    "        if step % params['generate_step'] == 0 and step > 1:\n",
    "            ints = sess.run(ops['generate'])\n",
    "            print('\\n'+''.join([params['idx2char'][i] for i in ints])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
