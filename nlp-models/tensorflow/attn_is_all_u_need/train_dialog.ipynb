{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"transformer.png\" width=\"300\">\n",
    "\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bunch import Bunch\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Bunch({\n",
    "    'source_max_len': 10,\n",
    "    'target_max_len': 10,\n",
    "    'min_freq': 50,\n",
    "    'hidden_units': 128,\n",
    "    'num_blocks': 2,\n",
    "    'num_heads': 8,\n",
    "    'num_heads': 8,\n",
    "    'dropout_rate': 0.1,\n",
    "    'batch_size': 64,\n",
    "    'position_encoding': 'param',\n",
    "    'activation': 'relu',\n",
    "    'tied_proj_weight': True,\n",
    "    'tied_embedding': False,\n",
    "    'label_smoothing': False,\n",
    "    'lr_decay_strategy': 'exp',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, source_path, target_path):\n",
    "        self.source_words = self.read_data(source_path)\n",
    "        self.target_words = self.read_data(target_path)\n",
    "\n",
    "        self.source_word2idx = self.build_index(self.source_words)\n",
    "        self.target_word2idx = self.build_index(self.target_words, is_target=True)\n",
    "\n",
    "    \n",
    "    def read_data(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "\n",
    "    def build_index(self, data, is_target=False):\n",
    "        chars = [char for line in data.split('\\n') for char in line]\n",
    "        chars = [char for char, freq in Counter(chars).items() if freq > args.min_freq]\n",
    "        if is_target:\n",
    "            symbols = ['<pad>','<start>','<end>','<unk>']\n",
    "            return {char: idx for idx, char in enumerate(symbols + chars)}\n",
    "        else:\n",
    "            symbols = ['<pad>','<unk>'] if not args.tied_embedding else ['<pad>','<start>','<end>','<unk>']\n",
    "            return {char: idx for idx, char in enumerate(symbols + chars)}\n",
    "\n",
    "\n",
    "    def pad(self, data, word2idx, max_len, is_target=False):\n",
    "        res = []\n",
    "        for line in data.split('\\n'):\n",
    "            temp_line = [word2idx.get(char, word2idx['<unk>']) for char in line]\n",
    "            if len(temp_line) >= max_len:\n",
    "                if is_target:\n",
    "                    temp_line = temp_line[:(max_len-1)] + [word2idx['<end>']]\n",
    "                else:\n",
    "                    temp_line = temp_line[:max_len]\n",
    "            if len(temp_line) < max_len:\n",
    "                if is_target:\n",
    "                    temp_line += ([word2idx['<end>']] + [word2idx['<pad>']]*(max_len-len(temp_line)-1)) \n",
    "                else:\n",
    "                    temp_line += [word2idx['<pad>']] * (max_len - len(temp_line))\n",
    "            res.append(temp_line)\n",
    "        return np.array(res)\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        source_idx = self.pad(self.source_words, self.source_word2idx, args.source_max_len)\n",
    "        target_idx = self.pad(self.target_words, self.target_word2idx, args.target_max_len, is_target=True)\n",
    "        return source_idx, target_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "\n",
    "    params_shape = inputs.get_shape()[-1:]\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    \n",
    "    outputs = gamma * normalized + beta\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def embed_seq(inputs, vocab_size=None, embed_dim=None, zero_pad=False, scale=False):\n",
    "    lookup_table = tf.get_variable('lookup_table', dtype=tf.float32, shape=[vocab_size, embed_dim])\n",
    "    if zero_pad:\n",
    "        lookup_table = tf.concat((tf.zeros([1, embed_dim]), lookup_table[1:, :]), axis=0)\n",
    "    outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "    if scale:\n",
    "        outputs = outputs * np.sqrt(embed_dim)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def multihead_attn(queries, keys, q_masks, k_masks, num_units=None, num_heads=8,\n",
    "        dropout_rate=args.dropout_rate, future_binding=False, reuse=False, activation=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      queries: A 3d tensor with shape of [N, T_q, C_q]\n",
    "      keys: A 3d tensor with shape of [N, T_k, C_k]\n",
    "    \"\"\"\n",
    "    if num_units is None:\n",
    "        num_units = queries.get_shape().as_list[-1]\n",
    "    T_q = queries.get_shape().as_list()[1]                                         # max time length of query\n",
    "    T_k = keys.get_shape().as_list()[1]                                            # max time length of key\n",
    "\n",
    "    Q = tf.layers.dense(queries, num_units, activation, reuse=reuse, name='Q')     # (N, T_q, C)\n",
    "    K_V = tf.layers.dense(keys, 2*num_units, activation, reuse=reuse, name='K_V')    \n",
    "    K, V = tf.split(K_V, 2, -1)        \n",
    "\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)                         # (h*N, T_q, C/h) \n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)                         # (h*N, T_k, C/h) \n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)                         # (h*N, T_k, C/h)\n",
    "\n",
    "    # Scaled Dot-Product\n",
    "    align = tf.matmul(Q_, tf.transpose(K_, [0,2,1]))                               # (h*N, T_q, T_k)\n",
    "    align = align / np.sqrt(K_.get_shape().as_list()[-1])                          # scale\n",
    "\n",
    "    # Key Masking\n",
    "    paddings = tf.fill(tf.shape(align), float('-inf'))                             # exp(-large) -> 0\n",
    "\n",
    "    key_masks = k_masks                                                            # (N, T_k)\n",
    "    key_masks = tf.tile(key_masks, [num_heads, 1])                                 # (h*N, T_k)\n",
    "    key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, T_q, 1])                 # (h*N, T_q, T_k)\n",
    "    align = tf.where(tf.equal(key_masks, 0), paddings, align)                      # (h*N, T_q, T_k)\n",
    "\n",
    "    if future_binding:\n",
    "        lower_tri = tf.ones([T_q, T_k])                                            # (T_q, T_k)\n",
    "        lower_tri = tf.linalg.LinearOperatorLowerTriangular(lower_tri).to_dense()  # (T_q, T_k)\n",
    "        masks = tf.tile(tf.expand_dims(lower_tri,0), [tf.shape(align)[0], 1, 1])   # (h*N, T_q, T_k)\n",
    "        align = tf.where(tf.equal(masks, 0), paddings, align)                      # (h*N, T_q, T_k)\n",
    "    \n",
    "    # Softmax\n",
    "    align = tf.nn.softmax(align)                                                   # (h*N, T_q, T_k)\n",
    "\n",
    "    # Query Masking\n",
    "    query_masks = tf.to_float(q_masks)                                             # (N, T_q)\n",
    "    query_masks = tf.tile(query_masks, [num_heads, 1])                             # (h*N, T_q)\n",
    "    query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, T_k])            # (h*N, T_q, T_k)\n",
    "    align *= query_masks                                                           # (h*N, T_q, T_k)\n",
    "\n",
    "    align = tf.layers.dropout(align, dropout_rate, training=(not reuse))           # (h*N, T_q, T_k)\n",
    "\n",
    "    # Weighted sum\n",
    "    outputs = tf.matmul(align, V_)                                                 # (h*N, T_q, C/h)\n",
    "    # Restore shape\n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)              # (N, T_q, C)\n",
    "    # Residual connection\n",
    "    outputs += queries                                                             # (N, T_q, C)   \n",
    "    # Normalize\n",
    "    outputs = layer_norm(outputs)                                                  # (N, T_q, C)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def pointwise_feedforward(inputs, num_units=[None, None], activation=None):\n",
    "    # Inner layer\n",
    "    outputs = tf.layers.conv1d(inputs, num_units[0], kernel_size=1, activation=activation)\n",
    "    # Readout layer\n",
    "    outputs = tf.layers.conv1d(outputs, num_units[1], kernel_size=1, activation=None)\n",
    "    # Residual connection\n",
    "    outputs += inputs\n",
    "    # Normalize\n",
    "    outputs = layer_norm(outputs)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def learned_position_encoding(inputs, mask, embed_dim):\n",
    "    T = inputs.get_shape().as_list()[1]\n",
    "    outputs = tf.range(tf.shape(inputs)[1])                # (T_q)\n",
    "    outputs = tf.expand_dims(outputs, 0)                   # (1, T_q)\n",
    "    outputs = tf.tile(outputs, [tf.shape(inputs)[0], 1])   # (N, T_q)\n",
    "    outputs = embed_seq(outputs, T, embed_dim, zero_pad=False, scale=False)\n",
    "    return tf.expand_dims(tf.to_float(mask), -1) * outputs\n",
    "\n",
    "\n",
    "def sinusoidal_position_encoding(inputs, mask, num_units):\n",
    "    T = inputs.get_shape().as_list()[1]\n",
    "    position_idx = tf.tile(tf.expand_dims(tf.range(T), 0), [tf.shape(inputs)[0], 1])\n",
    "\n",
    "    position_enc = np.array(\n",
    "        [[pos / np.power(10000, 2.*i/num_units) for i in range(num_units)] for pos in range(T)])\n",
    "    position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i\n",
    "    position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    lookup_table = tf.convert_to_tensor(position_enc, tf.float32)\n",
    "    outputs = tf.nn.embedding_lookup(lookup_table, position_idx)\n",
    "    \n",
    "    return tf.expand_dims(tf.to_float(mask), -1) * outputs\n",
    "\n",
    "\n",
    "def label_smoothing(inputs, epsilon=0.1):\n",
    "    C = inputs.get_shape().as_list()[-1] # number of channels\n",
    "    return ((1 - epsilon) * inputs) + (epsilon / C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(sources, targets, params, reuse=False):\n",
    "    with tf.variable_scope('forward_pass', reuse=reuse):\n",
    "        pos_enc = _get_position_encoder()\n",
    "\n",
    "        # ENCODER\n",
    "        en_masks = tf.sign(sources)   \n",
    "\n",
    "        with tf.variable_scope('encoder_embedding', reuse=reuse):\n",
    "            encoded = embed_seq(\n",
    "                sources, params['source_vocab_size'], args.hidden_units, zero_pad=True, scale=True)\n",
    "        \n",
    "        with tf.variable_scope('encoder_position_encoding', reuse=reuse):\n",
    "            encoded += pos_enc(sources, en_masks, args.hidden_units)\n",
    "        \n",
    "        with tf.variable_scope('encoder_dropout', reuse=reuse):\n",
    "            encoded = tf.layers.dropout(encoded, args.dropout_rate, training=(not reuse))\n",
    "\n",
    "        for i in range(args.num_blocks):\n",
    "            with tf.variable_scope('encoder_attn_%d'%i, reuse=reuse):\n",
    "                encoded = multihead_attn(queries=encoded, keys=encoded, q_masks=en_masks, k_masks=en_masks,\n",
    "                    num_units=args.hidden_units, num_heads=args.num_heads, dropout_rate=args.dropout_rate,\n",
    "                    future_binding=False, reuse=reuse, activation=None)\n",
    "            \n",
    "            with tf.variable_scope('encoder_feedforward_%d'%i, reuse=reuse):\n",
    "                encoded = pointwise_feedforward(encoded, num_units=[4*args.hidden_units, args.hidden_units],\n",
    "                    activation=params['activation'])\n",
    "\n",
    "        # DECODER\n",
    "        decoder_inputs = _shift_right(targets, params['start_symbol'])\n",
    "        de_masks = tf.sign(decoder_inputs)\n",
    "            \n",
    "        if args.tied_embedding:\n",
    "            with tf.variable_scope('encoder_embedding', reuse=True):\n",
    "                decoded = embed_seq(decoder_inputs, params['target_vocab_size'], args.hidden_units,\n",
    "                    zero_pad=True, scale=True)\n",
    "        else:\n",
    "            with tf.variable_scope('decoder_embedding', reuse=reuse):\n",
    "                decoded = embed_seq(\n",
    "                    decoder_inputs, params['target_vocab_size'], args.hidden_units, zero_pad=True, scale=True)\n",
    "        \n",
    "        with tf.variable_scope('decoder_position_encoding', reuse=reuse):\n",
    "            decoded += pos_enc(decoder_inputs, de_masks, args.hidden_units)\n",
    "                \n",
    "        with tf.variable_scope('decoder_dropout', reuse=reuse):\n",
    "            decoded = tf.layers.dropout(decoded, args.dropout_rate, training=(not reuse))\n",
    "\n",
    "        for i in range(args.num_blocks):\n",
    "            with tf.variable_scope('decoder_self_attn_%d'%i, reuse=reuse):\n",
    "                decoded = multihead_attn(queries=decoded, keys=decoded, q_masks=de_masks, k_masks=de_masks,\n",
    "                    num_units=args.hidden_units, num_heads=args.num_heads, dropout_rate=args.dropout_rate,\n",
    "                    future_binding=True, reuse=reuse, activation=None)\n",
    "            \n",
    "            with tf.variable_scope('decoder_attn_%d'%i, reuse=reuse):\n",
    "                decoded = multihead_attn(queries=decoded, keys=encoded, q_masks=de_masks, k_masks=en_masks,\n",
    "                    num_units=args.hidden_units, num_heads=args.num_heads, dropout_rate=args.dropout_rate,\n",
    "                    future_binding=False, reuse=reuse, activation=None)\n",
    "            \n",
    "            with tf.variable_scope('decoder_feedforward_%d'%i, reuse=reuse):\n",
    "                decoded = pointwise_feedforward(decoded, num_units=[4*args.hidden_units, args.hidden_units],\n",
    "                    activation=params['activation'])\n",
    "        \n",
    "        # OUTPUT LAYER    \n",
    "        if args.tied_proj_weight:\n",
    "            b = tf.get_variable('bias', [params['target_vocab_size']], tf.float32)\n",
    "            _scope = 'encoder_embedding' if args.tied_embedding else 'decoder_embedding'\n",
    "            with tf.variable_scope(_scope, reuse=True):\n",
    "                shared_w = tf.get_variable('lookup_table')\n",
    "            decoded = tf.reshape(decoded, [-1, args.hidden_units])\n",
    "            logits = tf.nn.xw_plus_b(decoded, tf.transpose(shared_w), b)\n",
    "            logits = tf.reshape(logits, [tf.shape(sources)[0], -1, params['target_vocab_size']])\n",
    "        else:\n",
    "            with tf.variable_scope('output_layer', reuse=reuse):\n",
    "                logits = tf.layers.dense(decoded, params['target_vocab_size'], reuse=reuse)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def _model_fn_train(features, mode, params, logits):\n",
    "    with tf.name_scope('backward'):\n",
    "        targets = features['target']\n",
    "        masks = tf.to_float(tf.not_equal(targets, 0))\n",
    "\n",
    "        if args.label_smoothing:\n",
    "            loss_op = label_smoothing_sequence_loss(\n",
    "                logits=logits, targets=targets, weights=masks, label_depth=params['target_vocab_size'])\n",
    "        else:\n",
    "            loss_op = tf.contrib.seq2seq.sequence_loss(\n",
    "                logits=logits, targets=targets, weights=masks)\n",
    "\n",
    "        if args.lr_decay_strategy == 'noam':\n",
    "            step_num = tf.train.get_global_step() + 1   # prevents zero global step\n",
    "            lr = _get_noam_lr(step_num)\n",
    "        elif args.lr_decay_strategy == 'exp':\n",
    "            lr = tf.train.exponential_decay(1e-3, tf.train.get_global_step(), 100000, 0.1)\n",
    "        else:\n",
    "            raise ValueError(\"lr decay strategy must be one of 'noam' and 'exp'\")\n",
    "        log_hook = tf.train.LoggingTensorHook({'lr': lr}, every_n_iter=100)\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(lr).minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss_op, train_op=train_op, training_hooks=[log_hook])\n",
    "\n",
    "\n",
    "def _model_fn_predict(features, mode, params):\n",
    "    def cond(i, x, temp):\n",
    "        return i < args.target_max_len\n",
    "\n",
    "    def body(i, x, temp):\n",
    "        logits = forward_pass(features['source'], x, params, reuse=True)\n",
    "        ids = tf.argmax(logits, -1)[:, i]\n",
    "        ids = tf.expand_dims(ids, -1)\n",
    "\n",
    "        temp = tf.concat([temp[:, 1:], ids], -1)\n",
    "\n",
    "        x = tf.concat([temp[:, -(i+1):], temp[:, :-(i+1)]], -1)\n",
    "        x = tf.reshape(x, [tf.shape(temp)[0], args.target_max_len])\n",
    "        i += 1\n",
    "        return i, x, temp\n",
    "\n",
    "    _, res, _ = tf.while_loop(cond, body, [tf.constant(0), features['target'], features['target']])\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=res)\n",
    "\n",
    "\n",
    "def tf_estimator_model_fn(features, labels, mode, params):\n",
    "    logits = forward_pass(features['source'], features['target'], params)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        _ = forward_pass(features['source'], features['target'], params, reuse=True)\n",
    "        return _model_fn_train(features, mode, params, logits)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return _model_fn_predict(features, mode, params)\n",
    "\n",
    "\n",
    "def _shift_right(targets, start_symbol):\n",
    "    start_symbols = tf.cast(tf.fill([tf.shape(targets)[0], 1], start_symbol), tf.int64)\n",
    "    return tf.concat([start_symbols, targets[:, :-1]], axis=-1)\n",
    "\n",
    "\n",
    "def _get_position_encoder():\n",
    "    if args.position_encoding == 'non_param':\n",
    "        pos_enc = sinusoidal_position_encoding\n",
    "    elif args.position_encoding == 'param':\n",
    "        pos_enc = learned_position_encoding\n",
    "    else:\n",
    "        raise ValueError(\"position encoding has to be either 'param' or 'non_param'\")\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "def _get_noam_lr(step_num):\n",
    "    return tf.rsqrt(tf.to_float(args.hidden_units)) * tf.minimum(\n",
    "        tf.rsqrt(tf.to_float(step_num)),\n",
    "        tf.to_float(step_num) * tf.convert_to_tensor(args.warmup_steps ** (-1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocab Size: 2022\n",
      "Target Vocab Size: 2481\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1172422e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2/model.ckpt.\n",
      "INFO:tensorflow:loss = 7.817753, step = 1\n",
      "INFO:tensorflow:lr = 0.001\n",
      "INFO:tensorflow:global_step/sec: 5.81881\n",
      "INFO:tensorflow:loss = 4.960444, step = 101 (17.187 sec)\n",
      "INFO:tensorflow:lr = 0.0009977 (17.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.98515\n",
      "INFO:tensorflow:loss = 4.679043, step = 201 (16.708 sec)\n",
      "INFO:tensorflow:lr = 0.0009954055 (16.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.89046\n",
      "INFO:tensorflow:loss = 4.7163434, step = 301 (16.977 sec)\n",
      "INFO:tensorflow:lr = 0.0009931161 (16.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.14872\n",
      "INFO:tensorflow:loss = 4.2421327, step = 401 (16.263 sec)\n",
      "INFO:tensorflow:lr = 0.000990832 (16.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.17433\n",
      "INFO:tensorflow:loss = 4.2411776, step = 501 (16.196 sec)\n",
      "INFO:tensorflow:lr = 0.0009885532 (16.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.17781\n",
      "INFO:tensorflow:loss = 4.3803473, step = 601 (16.187 sec)\n",
      "INFO:tensorflow:lr = 0.0009862796 (16.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.24694\n",
      "INFO:tensorflow:loss = 3.8869967, step = 701 (16.008 sec)\n",
      "INFO:tensorflow:lr = 0.0009840112 (16.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.50967\n",
      "INFO:tensorflow:loss = 4.011197, step = 801 (15.362 sec)\n",
      "INFO:tensorflow:lr = 0.000981748 (15.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.56654\n",
      "INFO:tensorflow:loss = 4.1305633, step = 901 (15.229 sec)\n",
      "INFO:tensorflow:lr = 0.00097949 (15.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.12449\n",
      "INFO:tensorflow:loss = 3.8289816, step = 1001 (16.328 sec)\n",
      "INFO:tensorflow:lr = 0.0009772372 (16.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.43336\n",
      "INFO:tensorflow:loss = 3.4262736, step = 1101 (15.544 sec)\n",
      "INFO:tensorflow:lr = 0.00097498967 (15.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.50969\n",
      "INFO:tensorflow:loss = 3.6934392, step = 1201 (15.362 sec)\n",
      "INFO:tensorflow:lr = 0.0009727473 (15.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.57822\n",
      "INFO:tensorflow:loss = 3.6041377, step = 1301 (15.201 sec)\n",
      "INFO:tensorflow:lr = 0.00097051 (15.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.69179\n",
      "INFO:tensorflow:loss = 3.8620558, step = 1401 (14.944 sec)\n",
      "INFO:tensorflow:lr = 0.0009682779 (14.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.70696\n",
      "INFO:tensorflow:loss = 3.679435, step = 1501 (14.910 sec)\n",
      "INFO:tensorflow:lr = 0.0009660509 (14.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.39661\n",
      "INFO:tensorflow:loss = 3.6396224, step = 1601 (15.633 sec)\n",
      "INFO:tensorflow:lr = 0.0009638291 (15.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.49137\n",
      "INFO:tensorflow:loss = 3.3692431, step = 1701 (15.405 sec)\n",
      "INFO:tensorflow:lr = 0.0009616123 (15.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.45717\n",
      "INFO:tensorflow:loss = 3.515416, step = 1801 (15.487 sec)\n",
      "INFO:tensorflow:lr = 0.0009594007 (15.487 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.41239\n",
      "INFO:tensorflow:loss = 3.4216244, step = 1901 (15.595 sec)\n",
      "INFO:tensorflow:lr = 0.00095719413 (15.595 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.32672\n",
      "INFO:tensorflow:loss = 3.4024794, step = 2001 (15.805 sec)\n",
      "INFO:tensorflow:lr = 0.00095499266 (15.805 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.44181\n",
      "INFO:tensorflow:loss = 3.215161, step = 2101 (15.523 sec)\n",
      "INFO:tensorflow:lr = 0.0009527962 (15.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.4936\n",
      "INFO:tensorflow:loss = 3.1295807, step = 2201 (15.400 sec)\n",
      "INFO:tensorflow:lr = 0.00095060485 (15.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.55802\n",
      "INFO:tensorflow:loss = 3.3131843, step = 2301 (15.249 sec)\n",
      "INFO:tensorflow:lr = 0.00094841846 (15.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.56651\n",
      "INFO:tensorflow:loss = 3.7672813, step = 2401 (15.229 sec)\n",
      "INFO:tensorflow:lr = 0.0009462372 (15.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.41156\n",
      "INFO:tensorflow:loss = 3.3401725, step = 2501 (15.597 sec)\n",
      "INFO:tensorflow:lr = 0.0009440609 (15.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.55187\n",
      "INFO:tensorflow:loss = 3.488318, step = 2601 (15.262 sec)\n",
      "INFO:tensorflow:lr = 0.00094188965 (15.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.54054\n",
      "INFO:tensorflow:loss = 3.2181232, step = 2701 (15.289 sec)\n",
      "INFO:tensorflow:lr = 0.00093972334 (15.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.54766\n",
      "INFO:tensorflow:loss = 2.9741654, step = 2801 (15.272 sec)\n",
      "INFO:tensorflow:lr = 0.000937562 (15.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.55554\n",
      "INFO:tensorflow:loss = 3.057877, step = 2901 (15.254 sec)\n",
      "INFO:tensorflow:lr = 0.0009354057 (15.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.34156\n",
      "INFO:tensorflow:loss = 3.2052393, step = 3001 (15.769 sec)\n",
      "INFO:tensorflow:lr = 0.00093325437 (15.769 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.20782\n",
      "INFO:tensorflow:loss = 2.9597118, step = 3101 (16.109 sec)\n",
      "INFO:tensorflow:lr = 0.0009311079 (16.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09241\n",
      "INFO:tensorflow:loss = 3.4254198, step = 3201 (16.414 sec)\n",
      "INFO:tensorflow:lr = 0.00092896644 (16.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.26195\n",
      "INFO:tensorflow:loss = 3.4007175, step = 3301 (15.970 sec)\n",
      "INFO:tensorflow:lr = 0.0009268299 (15.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.51632\n",
      "INFO:tensorflow:loss = 2.904978, step = 3401 (15.346 sec)\n",
      "INFO:tensorflow:lr = 0.0009246982 (15.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.52398\n",
      "INFO:tensorflow:loss = 3.0725787, step = 3501 (15.328 sec)\n",
      "INFO:tensorflow:lr = 0.00092257146 (15.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.57777\n",
      "INFO:tensorflow:loss = 3.2743092, step = 3601 (15.203 sec)\n",
      "INFO:tensorflow:lr = 0.0009204496 (15.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.63019\n",
      "INFO:tensorflow:loss = 3.5528426, step = 3701 (15.082 sec)\n",
      "INFO:tensorflow:lr = 0.0009183326 (15.083 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3798 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 5.01562\n",
      "INFO:tensorflow:loss = 3.1829352, step = 3801 (19.938 sec)\n",
      "INFO:tensorflow:lr = 0.00091622054 (19.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.69185\n",
      "INFO:tensorflow:loss = 2.875205, step = 3901 (14.943 sec)\n",
      "INFO:tensorflow:lr = 0.00091411325 (14.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.70117\n",
      "INFO:tensorflow:loss = 2.7294002, step = 4001 (14.923 sec)\n",
      "INFO:tensorflow:lr = 0.0009120109 (14.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.70009\n",
      "INFO:tensorflow:loss = 2.4862378, step = 4101 (14.925 sec)\n",
      "INFO:tensorflow:lr = 0.00090991333 (14.925 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.66731\n",
      "INFO:tensorflow:loss = 2.8355913, step = 4201 (14.999 sec)\n",
      "INFO:tensorflow:lr = 0.0009078206 (14.998 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.68739\n",
      "INFO:tensorflow:loss = 2.9606786, step = 4301 (14.954 sec)\n",
      "INFO:tensorflow:lr = 0.0009057327 (14.954 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.68882\n",
      "INFO:tensorflow:loss = 2.8857522, step = 4401 (14.950 sec)\n",
      "INFO:tensorflow:lr = 0.0009036495 (14.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.14096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 3.2124953, step = 4501 (16.284 sec)\n",
      "INFO:tensorflow:lr = 0.0009015712 (16.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.05663\n",
      "INFO:tensorflow:loss = 2.7760828, step = 4601 (16.511 sec)\n",
      "INFO:tensorflow:lr = 0.0008994976 (16.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.26572\n",
      "INFO:tensorflow:loss = 3.0912976, step = 4701 (15.960 sec)\n",
      "INFO:tensorflow:lr = 0.00089742884 (15.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.2609\n",
      "INFO:tensorflow:loss = 3.0268857, step = 4801 (15.972 sec)\n",
      "INFO:tensorflow:lr = 0.0008953648 (15.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.56058\n",
      "INFO:tensorflow:loss = 3.069369, step = 4901 (15.242 sec)\n",
      "INFO:tensorflow:lr = 0.0008933055 (15.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.54205\n",
      "INFO:tensorflow:loss = 2.7635202, step = 5001 (15.286 sec)\n",
      "INFO:tensorflow:lr = 0.000891251 (15.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.4039\n",
      "INFO:tensorflow:loss = 3.2860072, step = 5101 (15.616 sec)\n",
      "INFO:tensorflow:lr = 0.0008892011 (15.616 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.44934\n",
      "INFO:tensorflow:loss = 3.74089, step = 5201 (15.505 sec)\n",
      "INFO:tensorflow:lr = 0.00088715606 (15.505 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.57678\n",
      "INFO:tensorflow:loss = 2.7664366, step = 5301 (15.205 sec)\n",
      "INFO:tensorflow:lr = 0.00088511565 (15.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.61293\n",
      "INFO:tensorflow:loss = 2.7033644, step = 5401 (15.123 sec)\n",
      "INFO:tensorflow:lr = 0.00088307995 (15.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.73496\n",
      "INFO:tensorflow:loss = 3.2973435, step = 5501 (14.847 sec)\n",
      "INFO:tensorflow:lr = 0.0008810489 (14.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.22829\n",
      "INFO:tensorflow:loss = 3.3070717, step = 5601 (16.056 sec)\n",
      "INFO:tensorflow:lr = 0.0008790226 (16.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46175\n",
      "INFO:tensorflow:loss = 2.5585406, step = 5701 (15.475 sec)\n",
      "INFO:tensorflow:lr = 0.00087700086 (15.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.54736\n",
      "INFO:tensorflow:loss = 2.7462063, step = 5801 (15.274 sec)\n",
      "INFO:tensorflow:lr = 0.00087498385 (15.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.55166\n",
      "INFO:tensorflow:loss = 3.0175066, step = 5901 (15.263 sec)\n",
      "INFO:tensorflow:lr = 0.0008729714 (15.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.65925\n",
      "INFO:tensorflow:loss = 2.6899793, step = 6001 (15.017 sec)\n",
      "INFO:tensorflow:lr = 0.0008709636 (15.017 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.71379\n",
      "INFO:tensorflow:loss = 2.9391477, step = 6101 (14.895 sec)\n",
      "INFO:tensorflow:lr = 0.00086896046 (14.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.71623\n",
      "INFO:tensorflow:loss = 3.232261, step = 6201 (14.889 sec)\n",
      "INFO:tensorflow:lr = 0.00086696196 (14.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.69798\n",
      "INFO:tensorflow:loss = 2.986, step = 6301 (14.930 sec)\n",
      "INFO:tensorflow:lr = 0.000864968 (14.930 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.7194\n",
      "INFO:tensorflow:loss = 3.1321778, step = 6401 (14.882 sec)\n",
      "INFO:tensorflow:lr = 0.00086297863 (14.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46105\n",
      "INFO:tensorflow:loss = 3.2480633, step = 6501 (15.477 sec)\n",
      "INFO:tensorflow:lr = 0.0008609938 (15.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.54189\n",
      "INFO:tensorflow:loss = 3.43955, step = 6601 (15.286 sec)\n",
      "INFO:tensorflow:lr = 0.0008590135 (15.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.52384\n",
      "INFO:tensorflow:loss = 3.6497626, step = 6701 (15.328 sec)\n",
      "INFO:tensorflow:lr = 0.0008570379 (15.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.65051\n",
      "INFO:tensorflow:loss = 3.6450722, step = 6801 (15.036 sec)\n",
      "INFO:tensorflow:lr = 0.00085506676 (15.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.72738\n",
      "INFO:tensorflow:loss = 3.556276, step = 6901 (14.865 sec)\n",
      "INFO:tensorflow:lr = 0.00085310015 (14.864 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.67856\n",
      "INFO:tensorflow:loss = 3.8510928, step = 7001 (14.973 sec)\n",
      "INFO:tensorflow:lr = 0.0008511381 (14.974 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7092 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.2073035.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2/model.ckpt-7092\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "你是谁 -> 我是小通\n",
      "你喜欢我吗 -> 我也喜欢你\n",
      "给我唱一首歌 -> 我是小通\n",
      "我帅吗 -> 你是个大傻逼\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2/model.ckpt-7092\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 7093 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.2250085, step = 7093\n",
      "INFO:tensorflow:lr = 0.00084933697\n",
      "INFO:tensorflow:global_step/sec: 5.98387\n",
      "INFO:tensorflow:loss = 3.0883234, step = 7193 (16.713 sec)\n",
      "INFO:tensorflow:lr = 0.0008473835 (16.713 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.57389\n",
      "INFO:tensorflow:loss = 3.033348, step = 7293 (15.212 sec)\n",
      "INFO:tensorflow:lr = 0.00084543467 (15.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.43995\n",
      "INFO:tensorflow:loss = 3.3937197, step = 7393 (15.528 sec)\n",
      "INFO:tensorflow:lr = 0.0008434902 (15.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.39371\n",
      "INFO:tensorflow:loss = 3.0044775, step = 7493 (15.641 sec)\n",
      "INFO:tensorflow:lr = 0.00084155024 (15.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46867\n",
      "INFO:tensorflow:loss = 2.9142408, step = 7593 (15.459 sec)\n",
      "INFO:tensorflow:lr = 0.00083961466 (15.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46511\n",
      "INFO:tensorflow:loss = 3.3394058, step = 7693 (15.468 sec)\n",
      "INFO:tensorflow:lr = 0.0008376836 (15.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.48593\n",
      "INFO:tensorflow:loss = 3.184004, step = 7793 (15.418 sec)\n",
      "INFO:tensorflow:lr = 0.000835757 (15.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.44133\n",
      "INFO:tensorflow:loss = 3.5130532, step = 7893 (15.524 sec)\n",
      "INFO:tensorflow:lr = 0.0008338348 (15.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.55247\n",
      "INFO:tensorflow:loss = 2.5794802, step = 7993 (15.261 sec)\n",
      "INFO:tensorflow:lr = 0.00083191704 (15.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.55035\n",
      "INFO:tensorflow:loss = 2.8611443, step = 8093 (15.266 sec)\n",
      "INFO:tensorflow:lr = 0.0008300037 (15.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.61815\n",
      "INFO:tensorflow:loss = 2.9064353, step = 8193 (15.110 sec)\n",
      "INFO:tensorflow:lr = 0.0008280948 (15.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.69099\n",
      "INFO:tensorflow:loss = 2.8487575, step = 8293 (14.946 sec)\n",
      "INFO:tensorflow:lr = 0.00082619017 (14.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.55441\n",
      "INFO:tensorflow:loss = 2.6871963, step = 8393 (15.257 sec)\n",
      "INFO:tensorflow:lr = 0.00082429004 (15.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.36285\n",
      "INFO:tensorflow:loss = 2.5606406, step = 8493 (15.716 sec)\n",
      "INFO:tensorflow:lr = 0.00082239415 (15.716 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.12269\n",
      "INFO:tensorflow:loss = 2.7432153, step = 8593 (16.333 sec)\n",
      "INFO:tensorflow:lr = 0.00082050276 (16.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.72645\n",
      "INFO:tensorflow:loss = 2.8916936, step = 8693 (17.463 sec)\n",
      "INFO:tensorflow:lr = 0.0008186156 (17.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.02919\n",
      "INFO:tensorflow:loss = 2.4093626, step = 8793 (16.586 sec)\n",
      "INFO:tensorflow:lr = 0.0008167329 (16.586 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.09396\n",
      "INFO:tensorflow:loss = 2.7072794, step = 8893 (16.409 sec)\n",
      "INFO:tensorflow:lr = 0.0008148544 (16.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.26247\n",
      "INFO:tensorflow:loss = 2.2230163, step = 8993 (15.968 sec)\n",
      "INFO:tensorflow:lr = 0.0008129803 (15.968 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.15432\n",
      "INFO:tensorflow:loss = 2.3950658, step = 9093 (16.249 sec)\n",
      "INFO:tensorflow:lr = 0.0008111105 (16.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.82199\n",
      "INFO:tensorflow:loss = 2.8585272, step = 9193 (17.176 sec)\n",
      "INFO:tensorflow:lr = 0.00080924504 (17.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.98844\n",
      "INFO:tensorflow:loss = 2.4663932, step = 9293 (16.699 sec)\n",
      "INFO:tensorflow:lr = 0.0008073838 (16.699 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.03506\n",
      "INFO:tensorflow:loss = 2.6713564, step = 9393 (16.570 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:lr = 0.00080552686 (16.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.13069\n",
      "INFO:tensorflow:loss = 3.0732715, step = 9493 (16.312 sec)\n",
      "INFO:tensorflow:lr = 0.0008036742 (16.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.38792\n",
      "INFO:tensorflow:loss = 2.81342, step = 9593 (15.654 sec)\n",
      "INFO:tensorflow:lr = 0.0008018258 (15.654 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.06807\n",
      "INFO:tensorflow:loss = 3.0305955, step = 9693 (16.479 sec)\n",
      "INFO:tensorflow:lr = 0.00079998164 (16.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.19815\n",
      "INFO:tensorflow:loss = 3.2607522, step = 9793 (16.134 sec)\n",
      "INFO:tensorflow:lr = 0.00079814176 (16.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.15417\n",
      "INFO:tensorflow:loss = 2.7144303, step = 9893 (16.249 sec)\n",
      "INFO:tensorflow:lr = 0.00079630606 (16.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.46069\n",
      "INFO:tensorflow:loss = 3.2435334, step = 9993 (15.478 sec)\n",
      "INFO:tensorflow:lr = 0.0007944746 (15.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.33906\n",
      "INFO:tensorflow:loss = 3.1968224, step = 10093 (15.775 sec)\n",
      "INFO:tensorflow:lr = 0.00079264736 (15.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.17664\n",
      "INFO:tensorflow:loss = 2.7672215, step = 10193 (16.190 sec)\n",
      "INFO:tensorflow:lr = 0.00079082436 (16.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.19337\n",
      "INFO:tensorflow:loss = 2.6657367, step = 10293 (16.147 sec)\n",
      "INFO:tensorflow:lr = 0.0007890055 (16.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.34035\n",
      "INFO:tensorflow:loss = 2.6186063, step = 10393 (15.772 sec)\n",
      "INFO:tensorflow:lr = 0.00078719086 (15.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.35311\n",
      "INFO:tensorflow:loss = 2.2320743, step = 10493 (15.740 sec)\n",
      "INFO:tensorflow:lr = 0.00078538037 (15.740 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.43275\n",
      "INFO:tensorflow:loss = 2.582237, step = 10593 (15.546 sec)\n",
      "INFO:tensorflow:lr = 0.000783574 (15.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.4146\n",
      "INFO:tensorflow:loss = 2.3814547, step = 10693 (15.589 sec)\n",
      "INFO:tensorflow:lr = 0.00078177184 (15.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.30041\n",
      "INFO:tensorflow:loss = 2.386706, step = 10793 (15.872 sec)\n",
      "INFO:tensorflow:lr = 0.00077997387 (15.871 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10828 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 4.67451\n",
      "INFO:tensorflow:loss = 2.948728, step = 10893 (21.393 sec)\n",
      "INFO:tensorflow:lr = 0.0007781799 (21.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.36313\n",
      "INFO:tensorflow:loss = 2.7807386, step = 10993 (15.715 sec)\n",
      "INFO:tensorflow:lr = 0.0007763902 (15.716 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.45373\n",
      "INFO:tensorflow:loss = 3.10492, step = 11093 (15.495 sec)\n",
      "INFO:tensorflow:lr = 0.00077460456 (15.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.04197\n",
      "INFO:tensorflow:loss = 3.0811062, step = 11193 (16.551 sec)\n",
      "INFO:tensorflow:lr = 0.00077282294 (16.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.50724\n",
      "INFO:tensorflow:loss = 3.3898616, step = 11293 (15.367 sec)\n",
      "INFO:tensorflow:lr = 0.00077104557 (15.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.34232\n",
      "INFO:tensorflow:loss = 3.072984, step = 11393 (15.767 sec)\n",
      "INFO:tensorflow:lr = 0.0007692722 (15.767 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.2885\n",
      "INFO:tensorflow:loss = 3.0446286, step = 11493 (15.903 sec)\n",
      "INFO:tensorflow:lr = 0.0007675029 (15.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.39939\n",
      "INFO:tensorflow:loss = 2.7106347, step = 11593 (15.626 sec)\n",
      "INFO:tensorflow:lr = 0.00076573767 (15.626 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.14701\n",
      "INFO:tensorflow:loss = 3.199056, step = 11693 (16.268 sec)\n",
      "INFO:tensorflow:lr = 0.00076397654 (16.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.24099\n",
      "INFO:tensorflow:loss = 3.3020217, step = 11793 (16.023 sec)\n",
      "INFO:tensorflow:lr = 0.0007622195 (16.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.45916\n",
      "INFO:tensorflow:loss = 3.476397, step = 11893 (15.482 sec)\n",
      "INFO:tensorflow:lr = 0.0007604664 (15.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.2857\n",
      "INFO:tensorflow:loss = 2.8696952, step = 11993 (15.909 sec)\n",
      "INFO:tensorflow:lr = 0.00075871736 (15.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.37372\n",
      "INFO:tensorflow:loss = 3.35639, step = 12093 (15.690 sec)\n",
      "INFO:tensorflow:lr = 0.00075697235 (15.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.10827\n",
      "INFO:tensorflow:loss = 3.1067348, step = 12193 (16.371 sec)\n",
      "INFO:tensorflow:lr = 0.00075523136 (16.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.24892\n",
      "INFO:tensorflow:loss = 3.213224, step = 12293 (16.003 sec)\n",
      "INFO:tensorflow:lr = 0.00075349445 (16.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.98976\n",
      "INFO:tensorflow:loss = 3.191794, step = 12393 (16.697 sec)\n",
      "INFO:tensorflow:lr = 0.0007517614 (16.697 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.154\n",
      "INFO:tensorflow:loss = 3.3233738, step = 12493 (16.248 sec)\n",
      "INFO:tensorflow:lr = 0.0007500324 (16.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.95683\n",
      "INFO:tensorflow:loss = 3.3763776, step = 12593 (16.787 sec)\n",
      "INFO:tensorflow:lr = 0.0007483074 (16.787 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.14629\n",
      "INFO:tensorflow:loss = 3.415699, step = 12693 (16.270 sec)\n",
      "INFO:tensorflow:lr = 0.0007465863 (16.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.2781\n",
      "INFO:tensorflow:loss = 3.0126636, step = 12793 (15.928 sec)\n",
      "INFO:tensorflow:lr = 0.00074486923 (15.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.24815\n",
      "INFO:tensorflow:loss = 3.4351356, step = 12893 (16.005 sec)\n",
      "INFO:tensorflow:lr = 0.00074315607 (16.005 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.13608\n",
      "INFO:tensorflow:loss = 3.2095168, step = 12993 (16.297 sec)\n",
      "INFO:tensorflow:lr = 0.0007414469 (16.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.48048\n",
      "INFO:tensorflow:loss = 3.2129617, step = 13093 (15.432 sec)\n",
      "INFO:tensorflow:lr = 0.0007397416 (15.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.59709\n",
      "INFO:tensorflow:loss = 2.8431685, step = 13193 (15.157 sec)\n",
      "INFO:tensorflow:lr = 0.00073804025 (15.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.54238\n",
      "INFO:tensorflow:loss = 3.0889182, step = 13293 (18.043 sec)\n",
      "INFO:tensorflow:lr = 0.00073634274 (18.043 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.63154\n",
      "INFO:tensorflow:loss = 3.1637745, step = 13393 (17.757 sec)\n",
      "INFO:tensorflow:lr = 0.0007346492 (17.757 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.99626\n",
      "INFO:tensorflow:loss = 2.9675899, step = 13493 (16.677 sec)\n",
      "INFO:tensorflow:lr = 0.00073295954 (16.677 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.42656\n",
      "INFO:tensorflow:loss = 2.705295, step = 13593 (15.560 sec)\n",
      "INFO:tensorflow:lr = 0.0007312738 (15.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.4339\n",
      "INFO:tensorflow:loss = 3.10737, step = 13693 (18.403 sec)\n",
      "INFO:tensorflow:lr = 0.00072959193 (18.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.28043\n",
      "INFO:tensorflow:loss = 3.0336378, step = 13793 (15.922 sec)\n",
      "INFO:tensorflow:lr = 0.0007279139 (15.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.36969\n",
      "INFO:tensorflow:loss = 2.9328659, step = 13893 (15.701 sec)\n",
      "INFO:tensorflow:lr = 0.00072623976 (15.701 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.29637\n",
      "INFO:tensorflow:loss = 2.8729594, step = 13993 (15.881 sec)\n",
      "INFO:tensorflow:lr = 0.0007245695 (15.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.19537\n",
      "INFO:tensorflow:loss = 2.6990154, step = 14093 (16.141 sec)\n",
      "INFO:tensorflow:lr = 0.000722903 (16.141 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14184 into /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.935881.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/sx/fv0r97j96fz8njp14dt5g7940000gn/T/tmp8632gsu2/model.ckpt-14184\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "你是谁 -> 我是小通\n",
      "你喜欢我吗 -> 我喜欢你\n",
      "给我唱一首歌 -> 我是小通\n",
      "我帅吗 -> 你是我最帅的小通\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(test_words, tf_estimator, dl):\n",
    "    test_indices = []\n",
    "    for test_word in test_words:\n",
    "        test_idx = [dl.source_word2idx[c] for c in test_word] + \\\n",
    "                   [dl.source_word2idx['<pad>']] * (args.source_max_len - len(test_word))\n",
    "        test_indices.append(test_idx)\n",
    "    test_indices = np.atleast_2d(test_indices)\n",
    "    \n",
    "    zeros = np.zeros([len(test_words), args.target_max_len], np.int64)\n",
    "\n",
    "    pred_ids = tf_estimator.predict(tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'source':test_indices, 'target':zeros}, batch_size=len(test_words), shuffle=False))\n",
    "    pred_ids = list(pred_ids)\n",
    "    \n",
    "    target_idx2word = {i: w for w, i in dl.target_word2idx.items()}\n",
    "    for i, test_word in enumerate(test_words):\n",
    "        ans = ''.join([target_idx2word[id] for id in pred_ids[i]])\n",
    "        print(test_word, '->', ans.replace('<end>', ''))\n",
    "\n",
    "\n",
    "def prepare_params(dl):\n",
    "    if args.activation == 'relu':\n",
    "        activation = tf.nn.relu\n",
    "    elif args.activation == 'elu':\n",
    "        activation = tf.nn.elu\n",
    "    elif args.activation == 'lrelu':\n",
    "        activation = tf.nn.leaky_relu\n",
    "    else:\n",
    "        raise ValueError(\"acitivation fn has to be 'relu' or 'elu' or 'lrelu'\")\n",
    "    params = {\n",
    "        'source_vocab_size': len(dl.source_word2idx),\n",
    "        'target_vocab_size': len(dl.target_word2idx),\n",
    "        'start_symbol': dl.target_word2idx['<start>'],\n",
    "        'activation': activation}\n",
    "    return params\n",
    "\n",
    "\n",
    "def main():\n",
    "    dl = DataLoader(\n",
    "        source_path='../temp/dialog_source.txt',\n",
    "        target_path='../temp/dialog_target.txt')\n",
    "    sources, targets = dl.load()\n",
    "    print('Source Vocab Size:', len(dl.source_word2idx))\n",
    "    print('Target Vocab Size:', len(dl.target_word2idx))\n",
    "    \n",
    "    tf_estimator = tf.estimator.Estimator(\n",
    "        tf_estimator_model_fn, params=prepare_params(dl))\n",
    "    \n",
    "    for epoch in range(2):\n",
    "        tf_estimator.train(tf.estimator.inputs.numpy_input_fn(\n",
    "            x = {'source':sources, 'target':targets},\n",
    "            batch_size = args.batch_size,\n",
    "            shuffle = True))\n",
    "        greedy_decode(['你是谁', '你喜欢我吗', '给我唱一首歌', '我帅吗'], tf_estimator, dl)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
